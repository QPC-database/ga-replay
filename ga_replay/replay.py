import csv, time, requests, os, sys, random, re, logging
from urllib.parse import urlencode
from math import ceil
from datetime import datetime, timedelta
from collections import OrderedDict

from numpy import random as numpy_random
import asyncio
from aiohttp import ClientSession

from ga_replay.analytics import analytics
import config

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.dirname(BASE_DIR)

logging.basicConfig(filename="urls.log", level=logging.INFO)

def _write_itinerary(itinerary, outfile_path):
    """
    Write itinerary to CSV file.
    """
    with open(outfile_path, "wt") as f:
        writer = csv.writer(f)
        writer.writerows(itinerary)

def _get_datetime(timestamp):
    return datetime.strptime(timestamp, "%Y-%m-%dT%H:%M:00")
    
def get_itinerary(start, end, sites, outfile_path=None, extra_dimensions=[]):
    """
    Generate a requests itinerary CSV, given a start date, end date, file path
    and extra dimensions.  The itinerary is generated by grabbing the requests
    logged in historical google analytics data between the dates provided.

    This will generate a CSV of format:
        `DATE,HOUR,MINUTE,SITE_DOMAIN,PATH,[extra_dimensions],PAGEVIEWS`

    Args:
        * `start` - `date` - the date for the itinerary to begin
        * `end` - `date` - the date for the itinerary to end
        * `sites` - `iterable` - iterable of site domain strings
        * `outfile_path` - `string` - file path of the CSV to write to
        * `extra_dimensions` - `iterable` - iterable of additional GA dimensions
            to request
    """
    if not outfile_path:
        outfile_path = os.path.join(PROJECT_ROOT, "itineraries", "itinerary.csv")
    site_itineraries = {}
    for site in sites:
        ga_id = config.GA_SITES[site]
        print("**** Grabbing itinerary for %s ****" % site)
        site_itinerary = analytics.get_itinerary(start=start, end=end, 
            ga_id=ga_id, extra_dimensions=extra_dimensions)
        site_itineraries[site] = site_itinerary
        print("**** DONE ****")
    print("**** Flattening site itineraries ****")
    flat_itinerary = []
    for site, itinerary in site_itineraries.items():
        for row in itinerary:
            # Transpose to format:
            # [hour, minute, site, path, extra_dimensions*, pageviews] 
            date_str = "%s %s:%s" % (row[1], row[2], row[3])
            dt = datetime.strptime(date_str, "%Y%m%d %H:%M")
            flat_row = [dt.isoformat(), site, row[0]]
            flat_row.extend(row[4:])
            flat_itinerary.append(flat_row)
    print("**** DONE ****")
    print("**** Sorting itinerary by request time ****")
    flat_itinerary.sort(key=lambda row: row[0])
    print("**** DONE ****")
    print("**** Writing final itinerary ****")
    _write_itinerary(flat_itinerary, outfile_path)
    print("**** DONE ****")

async def dummy_request(domain, path, extra_dimensions=[], realtime=True):
    """
    """
    print("Requesting %s %s %s" % (domain, path, extra_dimensions))

async def simple_request(domain, path, extra_dimensions, realtime=True):
    """
    Simply re-run the request against the same domain/path.
    """
    url = "http://%s%s" % (domain, path)
    async with ClientSession() as session:
        async with session.get(url) as response:
            response = await response.read()

def _get_analytics_section_eurogamer(path):
    if path.startswith('/articles'):
        return 'article'
    if path.startswith('/jobs'):
        return 'jobs'
    if path.startswith('/forum'):
        return 'forum'
    if path.startswith('/user'):
        return 'user'
    if path.startswith('/search'):
        return 'search'
    if path.startswith('/games'):
        return 'game'
    if path.startswith("/profiles") or path.startswith("/inbox"):
        return 'community'
    return "archive"

def _get_analytics_section_nlife(path):
    if path.startswith('/news') or path.startswith('/reviews'):
        return 'article'
    if path.startswith('/forum'):
        return 'forum'
    if path.startswith('/games'):
        return 'game'
    return 'archive'

def _get_analytics_section_prima(path):
    if re.match("^/games/[-\w]+/[-\w]+/[-\w]+$", path):
        return "article"
    if re.match("^/games/[-\w]+/guides", path):
        return "guide"
    if path.startswith('/shop'):
        return 'shop'
    if path.startswith('/account'):
        return 'community'
    if path.startswith('/games'):
        return 'game'
    return 'archive'

def _get_analytics_section_wordpress(path):
    if re.match("^/[0-9]{4}/[0-9]{2}/[0-9]{2}/.+", path):
        return "article"
    return 'archive'

def _get_analytics_section(path, domain):
    if domain.startswith('eurogamer') or domain.startswith('usgamer') or \
            domain.startswith('gamesindustry'):
        return _get_analytics_section_eurogamer(path)
    if domain.startswith('nintendolife'):
        return _get_analytics_section_nlife(path)
    if domain.startswith('prima'):
        return _get_analytics_section_prima(path)
    if domain.startswith('rockpapershotgun') or domain.startswith('vg247'):
        return _get_analytics_section_wordpress(path)

article_publish_times = {}
def _get_article_published(path, domain, origin=None):
    if not origin:
        origin = datetime.now().isoformat()
    full_path = domain + path
    try:
        return article_publish_times[full_path]
    except KeyError:
        article_publish_times[full_path] = origin
        return article_publish_times[full_path]

platform_distributions = [
    (30, "ps4"),
    (20, "xboxone"),
    (15, "pc"),
    (5, "ps3"),
    (5, "xbox360"),
    (10, "wiiu"),
    (10, "3ds"),
    (5, "nx"),
]
platform_choices = []
platform_probabilities = []
for prob, choice in platform_distributions:
    platform_choices.append(choice)
    platform_probabilities.append(prob * 0.01)

def _get_randomised_platforms():
    platforms = []
    platform_count = int(random.random() * 5)
    for i in range(platform_count):
        platform = numpy_random.choice(platform_choices, p=platform_probabilities)
        platforms.append(platform)
    return platforms

article_platforms = {}
def _get_article_platforms(path, domain):
    full_path = domain + path
    try:
        return article_platforms[full_path]
    except KeyError:
        article_platforms[full_path] = _get_randomised_platforms()
        return article_platforms[full_path]

async def analytics_request(domain, path, extra_dimensions, timestamp, realtime=True):
    """
    Call an analytics endpoint, expects extra_dimensions to contain a referrer.
    """
    analytics_host = config.ANALYTICS_HOST
    section = _get_analytics_section(path, domain)
    if realtime:
        published = _get_article_published(path, domain)
    else:
        published = _get_article_published(path, domain, origin=timestamp)
    referrer = extra_dimensions[0]
    data = {
        'url': path, 'language': 'en', 'site': domain, 
        'section': section, 'published': published
    }
    if referrer != "(not set)":
        data['referrer'] = referrer
    if not realtime:
        data['timestamp'] = timestamp
    if section == "article":
        platforms = _get_article_platforms(path, domain)
        data['platforms'] = '||'.join(platforms)
    url = "http://%s/api/v1/register_pageview" % analytics_host
    params = urlencode(data)
    full_url = '?'.join([url, params])
    logging.info(full_url)
    async with ClientSession() as session:
        async with session.get(full_url) as response:
            response = await response.read()

async def run_request(request_func, request, seconds=59, realtime=True):
    """
    """
    random_delay = random.random() * seconds
    timestamp_dt = _get_datetime(request[0]) + timedelta(seconds=random_delay)
    timestamp = timestamp_dt.isoformat()
    if realtime:
        await asyncio.sleep(random_delay)
    try:
        result = await request_func(domain=request[1], path=request[2], 
            extra_dimensions=request[3:-1], timestamp=timestamp, realtime=realtime)
    except Exception as e:
        print("X", end="")
        print(e)
        sys.stdout.flush()
        return
    print(".", end="")
    sys.stdout.flush()

REQUEST_FUNCTIONS = {
    "dummy": dummy_request,
    "simple": simple_request,
    "analytics": analytics_request,
}

def _load_itinerary(itinerary_path):
    """
    Given a path to an itinerary CSV, load it in to an OrderedDict of format:

    `{
        "2017-06-31T22:00:00": {
            "timestamp": "2017-06-31T22:00",
            "itinerary": [
                ["17", "00", "eurogamer.net", "/", "(direct)", 10],
                ...
            ],
            "total_pageviews": 600,
        }
    }`

    Each row in the itinerary list is a single request to call.
    """
    flat_itinerary = []
    with open(itinerary_path, "rt") as f:
        reader = csv.reader(f)
        flat_itinerary = list(reader)
    itinerary = OrderedDict({})
    total_pageviews = 0
    for row in flat_itinerary:
        key = row[0]
        pageviews = int(row[-1])
        total_pageviews += pageviews
        try:
            itinerary[key]['itinerary'].extend([row] * pageviews)
            itinerary[key]['total_pageviews'] += pageviews
        except KeyError:
            itinerary[key] = {
                'itinerary': [row] * pageviews, 
                'timestamp': row[0], 
                'total_pageviews': pageviews
            }
    print("Loaded itinerary of %s total pageviews" % total_pageviews)
    return itinerary

loop = asyncio.get_event_loop()

def _run_realtime(request_func, requests):
    request_tasks = []
    for request in requests:
        task = asyncio.ensure_future(
            run_request(request_func=request_func, request=request, seconds=59, realtime=True)
        )
        request_tasks.append(task)
    loop.run_until_complete(asyncio.wait(request_tasks))

def _run_nonstop(request_func, requests, concurrency):
    batches = ceil(len(requests) / concurrency)
    for batch in range(1, batches + 1):
        start_index = (batch - 1) * concurrency
        end_index = start_index + concurrency
        request_tasks = []
        for request in requests[start_index:end_index]:
            task = asyncio.ensure_future(
                run_request(request_func=request_func, request=request, seconds=59, realtime=False)
            )
            request_tasks.append(task)
        loop.run_until_complete(asyncio.wait(request_tasks))

def simulate_from_itinerary(itinerary_path, request_func=dummy_request, start_time=None, realtime=True, concurrency=100):
    """
    Run the network requests in a given itinerary to replay traffic.

    Args:
        * `itinerary_path` - `string` - the path to the itinerary CSV file to replay
        * `[request_func]` - `function` - the function to use when making a request
        * `[start_time]` - `string` - string of format "YYYY-MM-DDTHH:MM:00" to indicate 
            what timestamp to start replaying the itinerary from
    """
    print("**** Loading itinerary... *****")
    itinerary = _load_itinerary(itinerary_path)
    print("**** Itinerary loaded! *****")
    all_itinerary_timestamps = list(itinerary.keys())
    if start_time:
        first_timestamp_index = all_itinerary_timestamps.index(start_time)
        all_itinerary_timestamps = all_itinerary_timestamps[first_timestamp_index:]

    timestamp = all_itinerary_timestamps.pop(0)
    simulation_dt = _get_datetime(timestamp)
    next_run = datetime.now()
    start = datetime.now()
    simulation_difference = start - simulation_dt
    print("**** Replaying traffic... ****")
    while True:
        loop_start = time.time()
        if realtime and datetime.now() < next_run:
            # It's not time to run the next timestamp in the itinerary yet, so hold off
            time.sleep(0.1)
            continue
        print()
        print("Replaying %s (%s pageviews) at %s" % \
            (itinerary[timestamp]['timestamp'], 
            itinerary[timestamp]['total_pageviews'],
            datetime.now())
        )
        requests = itinerary[timestamp]['itinerary']
        if realtime:
            _run_realtime(request_func, requests)
        else:
            _run_nonstop(request_func, requests, concurrency)
        # What's the next timestamp we need to move on to?
        try:
            next_timestamp = all_itinerary_timestamps.pop(0)
        except IndexError:
            break
        # When should we start running the itinerary for the next timestamp?
        timestamp = next_timestamp
        next_timestamp_dt = _get_datetime(next_timestamp)
        next_run = next_timestamp_dt + simulation_difference
        if not realtime:
            print("Replayed %s pageviews in %s seconds" % \
                (itinerary[timestamp]['total_pageviews'], ceil(time.time() - loop_start)))
    print("**** Done! ****")
